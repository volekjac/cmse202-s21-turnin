{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: right;\"> &#9989; Jake Volek</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment #5 (Individual)\n",
    "\n",
    "## Using SVMs and PCA with familiar data: The Iris Dataset\n",
    "\n",
    "### Goals for this homework assignment\n",
    "\n",
    "By the end of this assignment, you should be able to:\n",
    "* Use `git` to track your work and turn in your assignment\n",
    "* Read in data and prepare it for modeling\n",
    "* Build, fit, and evaluate an SVC model of data\n",
    "* Use PCA to reduce the number of important features\n",
    "* Build, fit, and evaluate an SVC model of PCA-transformed data\n",
    "* Systematically investigate the effects of the number of PCA components on an SVC model of data\n",
    "\n",
    "### Assignment instructions:\n",
    "\n",
    "Work through the following assignment, making sure to follow all of the directions and answer all of the questions.\n",
    "\n",
    "There are **44 points (+2 bonus points)** possible on this assignment. Point values for each part are included in the section headers.\n",
    "\n",
    "This assignment is **due at 11:59 pm on Friday, April 16. It should be pushed to your repo (see Part 1) and submitted to D2L**. \n",
    "\n",
    "#### Imports\n",
    "\n",
    "It's useful to put all of the imports you need for this assignment in one place. Read through the assignment to figure out which imports you'll need or add them here as you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all necessary imports here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "import time\n",
    "from sklearn.datasets import make_circles\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Add to your Git repository to track your progress on your assignment (4 points)\n",
    "\n",
    "As usual, for this assignment, you're going to add it to the `cmse202-s21-turnin` repository you created in class so that you can track your progress on the assignment and preserve the final version that you turn in. In order to do this you need to\n",
    "\n",
    "**&#9989; Do the following**:\n",
    "\n",
    "1. Navigate to your `cmse202-s21-turnin` repository and create a new directory called `hw-05`.\n",
    "2. Move this notebook into that **new directory** in your repository, then **add it and commit it to your repository**.\n",
    "1. Finally, to test that everything is working, \"git push\" the file so that it ends up in your GitHub repository.\n",
    "\n",
    "**Important**: Make sure you've added your Professor and your TA as collaborators to your \"turnin\" respository with \"Read\" access so that we can see your assignment (you should have done this in the previous homework assignment)\n",
    "\n",
    "**Also important**: Make sure that the version of this notebook that you are working on is the same one that you just added to your repository! If you are working on a different copy of the noteobok, **none of your changes will be tracked**!\n",
    "\n",
    "If everything went as intended, the file should now show up on your GitHub account in the \"`cmse202-s21-turnin`\" repository inside the `hw-05` directory that you just created.  Periodically, **you'll be asked to commit your changes to the repository and push them to the remote GitHub location**. Of course, you can always commit your changes more often than that, if you wish.  It can be good to get into a habit of committing your changes any time you make a significant modification, or when you stop working on the project for a bit.\n",
    "\n",
    "&#9989; **Do this**: Before you move on, put the command that your instructor should run to clone your repository in the markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "!git clone https://github.com/volekjac/cmse202-s21-turnin.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"loading\"></a>\n",
    "## 2. Loading a familiar dataset: The iris data (6 points)\n",
    "\n",
    "We've the seen the iris dataset a number of times in the course so far, and since the goal for this assignment is to practice using the SVM and PCA tools we've covered in class, we'll stick with using this simple dataset and avoid any complicated data wrangling headaches. As a reminder: you can find details about the dataset <a href=\"https://en.wikipedia.org/wiki/Iris_flower_data_set\">here</a>. \n",
    " \n",
    "#### The Iris data\n",
    "\n",
    "As we've seen, the iris data set is pretty straight forward. Rather than working with a perfectly curated data set though, we'll use the same version of the data that we first looked at during the **Day 6 in-class activity**.\n",
    "\n",
    "**&#9989; Do This:**  To get started, **you'll need to download the following two files** (or located them from when you used them previously):\n",
    "\n",
    "`https://raw.githubusercontent.com/msu-cmse-courses/cmse202-S21-student/master/data/iris.data`\n",
    "\n",
    "`https://raw.githubusercontent.com/msu-cmse-courses/cmse202-S21-student/master/data/iris.names`\n",
    "\n",
    "Once you've downloaded the data or copied it over from where your saved it previously, you should have access to the following : `iris.data` and `iris.names`.\n",
    "\n",
    "**Open the files using a text browser or other tool on your computer and confirm that they match your expectations and contain the data that we've worked with before.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the data\n",
    "\n",
    "**&#9989; Task 2.1 (2 points):** Read the ```iris.data``` file into your notebook **with appropriate column headers**. Since we are planning on classifying the data, you should label the fifth column `class`, which should have the iris species class labels:\n",
    "* \"Iris-setosa\"\n",
    "* \"Iris-versicolor\"\n",
    "* \"Iris-virginica\"\n",
    "\n",
    "Display the DataFrame to make sure it looks reasonable. You should have **5 columns** and **150 rows**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  4551  100  4551    0     0  42138      0 --:--:-- --:--:-- --:--:-- 42138\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3000  100  3000    0     0  27522      0 --:--:-- --:--:-- --:--:-- 27522\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "!curl -O https://raw.githubusercontent.com/msu-cmse-courses/cmse202-S21-student/master/data/iris.data\n",
    "!curl -O https://raw.githubusercontent.com/msu-cmse-courses/cmse202-S21-student/master/data/iris.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal, width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length  sepal width  petal length  petal, width           class\n",
       "0             5.1          3.5           1.4           0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4           0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3           0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5           0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4           0.2     Iris-setosa\n",
       "..            ...          ...           ...           ...             ...\n",
       "145           6.7          3.0           5.2           2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0           1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2           2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4           2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1           1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['sepal length', 'sepal width', 'petal length', 'petal, width', 'class']\n",
    "iris = pd.read_csv('iris.data', delimiter = ' ', names = column_names)\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Relabeling the classes\n",
    "\n",
    "To simplify the process of modeling the iris data, we should convert the class labels from strings to integers. For example, rather than `Iris-setosa`, we can consider this to be class \"`0`\".\n",
    "\n",
    "**&#9989; Task 2.2 (2 points):** Replace all of the strings in your \"class\" column with integers based on the following:\n",
    "\n",
    "| original label | replaced label |\n",
    "| -------- | -------- |\n",
    "| Iris-setosa | 0 |\n",
    "| Iris-versicolor | 1 |\n",
    "| Iris-virginica | 2 |\n",
    "\n",
    "Once you've replaced the labels, display your DataFrame and confirm that it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal, width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length  sepal width  petal length  petal, width  class\n",
       "0             5.1          3.5           1.4           0.2      0\n",
       "1             4.9          3.0           1.4           0.2      0\n",
       "2             4.7          3.2           1.3           0.2      0\n",
       "3             4.6          3.1           1.5           0.2      0\n",
       "4             5.0          3.6           1.4           0.2      0\n",
       "..            ...          ...           ...           ...    ...\n",
       "145           6.7          3.0           5.2           2.3      2\n",
       "146           6.3          2.5           5.0           1.9      2\n",
       "147           6.5          3.0           5.2           2.0      2\n",
       "148           6.2          3.4           5.4           2.3      2\n",
       "149           5.9          3.0           5.1           1.8      2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put your code here\n",
    "iris = iris.replace('Iris-setosa', 0)\n",
    "iris = iris.replace('Iris-versicolor', 1)\n",
    "iris = iris.replace('Iris-virginica', 2)\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Separating the \"features\" from the \"labels\"\n",
    "\n",
    "As we've seen when working with `sklearn` it can be much easier to work with the data if we have separate variables that store the features and the labels.\n",
    "\n",
    "**&#9989; Task 2.3 (1 point):** Split your DataFrame so that you have two separate DataFrames, one called `features`, which contains all of the iris features, and one called `labels`, which contains all of the *new* iris integer labels you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "features = iris.drop(['class'], axis = 1)\n",
    "labels = iris['class']\n",
    "zero_count = 0\n",
    "one_count = 0\n",
    "two_count = 0\n",
    "for label in labels:\n",
    "    if label == 0:\n",
    "        zero_count += 1\n",
    "    elif label == 1:\n",
    "        one_count += 1\n",
    "    elif label == 2:\n",
    "        two_count += 1\n",
    "print(zero_count, one_count, two_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 2.1 (1 point):** How balanced is your set of iris classes? Does it matter for the set of classes to be balanced? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our set of iris classes is perfectly balanced with 50 of each. This is good because it allows us to have an even distribution of classes when performing a model on our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 2\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Building an SVC model (4 points)\n",
    "\n",
    "Now, to tackle this classification problem, we will use a support vector machine just like we've done previously (e.g. in the **Day 19 and Day 20 assignments**). Of course, we could easily replace this with any `sklearn` classifier we choose, but for now we will just use an SVC with a linear kernel.\n",
    "\n",
    "### 3.1 Splitting the data\n",
    "\n",
    "But first, we need to split our data into training and testing data!\n",
    "\n",
    "**&#9989; Task 3.1 (1 point):** Split your data into a training and testing set with a training set representing 75% of your data. For reproducibility , set the `random_state` argument to `314159`. Print the lengths to show you have the right number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 38\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "train_vectors, test_vectors, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 314159)\n",
    "print(len(train_vectors), len(test_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Modeling the data and evaluating the fit\n",
    "\n",
    "As you have done this a number of times at this point, we ask you to do most of the analysis for this problem in one cell.\n",
    "\n",
    "**&#9989; Task 3.2 (2 points):** Build a linear SVC model with `C=0.01`, fit it to the training set, and use the test features to predict the outcomes. Evaluate the fit using the **confusion matrix** and **classification report**.\n",
    "\n",
    "**Note:** Double-check the documentation on the confusion matrix because the way `sklearn` outputs false positives and false negatives may be different from what most images on the web indicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  0  0]\n",
      " [ 0 11  1]\n",
      " [ 0  3  9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       0.79      0.92      0.85        12\n",
      "           2       0.90      0.75      0.82        12\n",
      "\n",
      "    accuracy                           0.89        38\n",
      "   macro avg       0.90      0.89      0.89        38\n",
      "weighted avg       0.90      0.89      0.89        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "pred = svm.SVC(C = 0.01, kernel = 'linear')\n",
    "pred = pred.fit(train_vectors, train_labels)\n",
    "pred1 = pred.predict(test_vectors)\n",
    "print(confusion_matrix(test_labels, pred1))\n",
    "print(classification_report(test_labels, pred1))\n",
    "#Rows = true labels\n",
    "#Columns = predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.1 (1 point):** How accurate is your model? What evidence are you using to determine that? How many false positives and false negatives does it predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our confusion matrix, we predicted fairly well. Our classification report shows that we predicted with 89% accuracy. We had 4 false predictions predicting 3 values that were actually class 2 into the class 1 group. The same thing happens when predicting 1 class 1 into the class 2 group of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 3\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Finding and using the best hyperparameters (8 points)\n",
    "\n",
    "At this point, we have fit one model and determined it's performance, but is it the best model? We can use `GridSearchCV` to find the best model (given our choices of parameters). Once we do that, we will use that best model going forward. This is similar to what we did when working with the \"digits\" data and the \"faces\" data in the **Day 20 and Day 21 assignments**.\n",
    "\n",
    "**Note:** you would typically rerun this grid search in a production environment to continue to verify the best model, but we are not for the sake of speed.\n",
    "\n",
    "### 4.1 Performing a grid search\n",
    "\n",
    "**&#9989; Task 4.1 (4 points):** Using the following parameters (`C` = `1e-3`, `0.01`, `0.1`, `1`, `10`, `100` and `gamma` = `1e-6`, `1e-5`, `1e-4`, `1e-3`, `0.01`, `0.1`) for both a `linear` and `rbf` kernel use `GridSearchCV` with the `SVC()` model to find the best fit parameters. Once, you're run the grid search, print the \"best estimators\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10, gamma=1e-06, kernel='linear')\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "svc = svm.SVC()\n",
    "param = {'kernel':('linear','rbf'), 'C':[1e-3, 0.01, 0.1, 1, 10, 100],\\\n",
    "         'gamma':[1e-6, 1e-5, 1e-4, 1e-3, 0.01, 0.1 ]}\n",
    "clf = GridSearchCV(svc, param)\n",
    "clf_fit = clf.fit(train_vectors, train_labels)\n",
    "print(clf_fit.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.1 (1 point):** How do the \"best estimator\" results of the grid search compare to what you used in Part 3? Did the hyper parameter(s) change? What kernel did the grid search determine was the best option? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C value changed from 0.01 to the best estimator being 10, but the kernel remained linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluating the best fit model\n",
    "\n",
    "Now that we have found the \"best estimators\", let's determine how good the fit is.\n",
    "\n",
    "**&#9989; Task 4.2 (2 points):** Use the test features to predict the outcomes for the best model. Evaluate the fit using the **confusion matrix** and **classification report**.\n",
    "\n",
    "**Note:** Double-check the documentation on the confusion matrix because the way `sklearn` outputs false positives and false negatives may be different from what most images on the web indicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  0  0]\n",
      " [ 0 10  2]\n",
      " [ 0  0 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      0.83      0.91        12\n",
      "           2       0.86      1.00      0.92        12\n",
      "\n",
      "    accuracy                           0.95        38\n",
      "   macro avg       0.95      0.94      0.94        38\n",
      "weighted avg       0.95      0.95      0.95        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "clf_pred = clf_fit.predict(test_vectors)\n",
    "print(confusion_matrix(test_labels, clf_pred))\n",
    "print(classification_report(test_labels, clf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.2 (1 point):** How accurate is this best model? What evidence are you using to determine that? How many false positives and false negatives does it predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This predicts better than the one in part 3. We see that we predict with 95% accuracy vs. the previous 89%. Our false positives/negatives dropped from 4 to 2 from part 3 to 4. Using the best estimators allows us to predict at a higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 4\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Using Principal Components (10 points)\n",
    "\n",
    "The full model uses all 4 iris features to predict the results and you likely found that the model is pretty accurate using all 4 features. But in some cases, we might have significantly more features (which means much more computational time!), and we might not need nearly the level of accuracy we can achieve with the full data set or we might not have enough computational resources to use **all** of the features.\n",
    "\n",
    "In such situations, we might need to see how close we can get with fewer features. But instead of simply removing features, we will use a PCA to determine the features that contribute the most the model (through their accounted variance) and use those to build our SVC model. We did this to improve our classification with the \"faces\" dataset in the **Day 21 assignment**.\n",
    "\n",
    "### 5.1 Running a Principle Component Analysis (PCA)\n",
    "\n",
    "Since we only have 4 total features to start with, let's see how well we can do if we try to cut this aggressively reduce the feature count and use only **1** principle component. We'll see how well we can predict the classes of the iris dataset with just these two!\n",
    "\n",
    "**&#9989; Task 5.1 (3 points):**  Using `PCA()` and the associated `fit()` method, run a principle component analysis to your training features using only 2 components. Transform both the test and training features using the result of your PCA. Print the `explained_variance_ratio_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9232169568032699\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "\n",
    "pca = PCA(n_components = 1)\n",
    "\n",
    "pca_train_fit = pca.fit(train_vectors)\n",
    "pca_train_vectors = pca.transform(train_vectors)\n",
    "print(pca.explained_variance_ratio_[0])\n",
    "\n",
    "\n",
    "pca_test_vectors = pca.transform(test_vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 5.1 (1 point):** What is the total explained variance ratio captured by this simple 1-component PCA? (just quote the number) How well do you think a model with this many feature will perform? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EVR value for this one component PCA is 0.9232169568032699. We would expect this model to predict correctly 92% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Fit and Evaluate an SVC model\n",
    "\n",
    "Using the pca transformed features, we will train and test an SVC model using the \"best estimators\" you found previously.\n",
    "\n",
    "**&#9989; Task 5.2 (2 points):**  Using the PCA transformed training data, build and train an SVC model using the best estimate values from before. Predict the classes using the PCA transformed test data. Evaluate the model using the classfication report, and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  0  0]\n",
      " [ 0  9  3]\n",
      " [ 0  0 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      0.75      0.86        12\n",
      "           2       0.80      1.00      0.89        12\n",
      "\n",
      "    accuracy                           0.92        38\n",
      "   macro avg       0.93      0.92      0.92        38\n",
      "weighted avg       0.94      0.92      0.92        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "new_svc = SVC(C = 10, gamma = 1e-06, kernel = 'linear')\n",
    "new_svc_fit = new_svc.fit(pca_train_vectors, train_labels)\n",
    "new_svc_pred = new_svc_fit.predict(pca_test_vectors)\n",
    "print(confusion_matrix(test_labels, new_svc_pred))\n",
    "print(classification_report(test_labels, new_svc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 5.2 (1 point):** How accurate is this model? What evidence are you using to determine that? How many false positives and false negatives does it predict? How does it compare to the full feature model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model predicts 92% correctly, with only 3 incorrect predictions. All 3 of these were predicted as class 2, but should have been in class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Repeat your analysis with more components\n",
    "\n",
    "You probably found that the model with just 1 features didn't actually do too bad, which is pretty impressive. That said, can we do better?\n",
    "\n",
    "What if we increase the number of principle components to **2**? What happens now?\n",
    "\n",
    "**&#9989; Task 5.3 (2 points):** Repeat your analysis from 5.1 and 5.2 using **2 components** instead. As part of your analysis, **print the total explained variance ratio for both components as well as the sum of these values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9776396024180721\n",
      "[[14  0  0]\n",
      " [ 0 11  1]\n",
      " [ 0  0 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      0.92      0.96        12\n",
      "           2       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.97      0.97      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "pca2 = PCA(n_components = 2)\n",
    "\n",
    "pca_train_fit2 = pca2.fit(train_vectors)\n",
    "pca_train_vectors2 = pca2.transform(train_vectors)\n",
    "pca_test_vectors2 = pca2.transform(test_vectors)\n",
    "\n",
    "print(np.sum(pca2.explained_variance_ratio_))\n",
    "\n",
    "new_svc2 = SVC(C = 10, gamma = 1e-06, kernel = 'linear')\n",
    "new_svc_fit2 = new_svc2.fit(pca_train_vectors2, train_labels)\n",
    "new_svc_pred2 = new_svc_fit2.predict(pca_test_vectors2)\n",
    "print(confusion_matrix(test_labels, new_svc_pred2))\n",
    "print(classification_report(test_labels, new_svc_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 5.3 (1 point):** What is the total explained variance ratio captured by this PCA? How accurate is this model? What evidence are you using to determine that? How many false positives and false negatives does it predict? How does it compare to the 1 PCA component model? To the full feature model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model predicted at 97% accuracy, with only 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 5\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. How well does PCA work? (12 points)\n",
    "\n",
    "Clearly, the number of components we use in our PCA matters. Let's investigate how they matter by systematically building a model for any number of selected components. While this might seem a bit unnecessary for such a simple dataset, **this can be very useful for more complex datasets and models!**\n",
    "\n",
    "### 6.1 Accuracy vs. Components\n",
    "\n",
    "To systematically explore how well PCA improves our classification model, we will do this by writing a function that creates the PCA, the SVC model, fits the training data, predict the labels using test data, and returns the accuracy scores and the explained variance ratio. So your function will take as input:\n",
    "* the number of request PCA components\n",
    "* the training feature data\n",
    "* the testing feature data\n",
    "* the training data labels\n",
    "* the test data labels\n",
    "and it will return the accuracy score for an SVC model fit to pca transformed features and the **total** explained variance ratio.\n",
    "\n",
    "**&#9989; Task 6.1 (4 points):** Create this function, which you will use in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Compute accuracies\n",
    "\n",
    "Now that you have created a function that returns the accuracy for a given number of components, we will use that to plot the how the accuracy of your SVC model changes when we increase the number of components used in the PCA.\n",
    "\n",
    "**&#9989; Task 6.2 (2 points):** For 1 through 4 components, use your function above to compute and store (as a list) the accuracy of your models and the total explained variance ratio of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Plot accuracy vs number of components\n",
    "\n",
    "Now that we have those numbers, it makes sense to look at the accuracy vs # of components.\n",
    "\n",
    "**&#9989; Task 6.3 (2 points):** Plot the accuracy vs # of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 6.1 (1 point):** Where does it seem like we have diminishing returns? That is, at what point is there no major increase in accuracy (or perhaps the accuracy is decreased) as we add additional components to the PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Erase this and put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Plot total explained variance vs number of components\n",
    "\n",
    "What if we look at total explained variance as a function of # of components?\n",
    "\n",
    "**&#9989; Task 6.4 (2 points):** Plot the total explained variance ratio vs # of components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 6.2 (1 points):** Where does it seem like we have diminishing returns, that is, no major increase in explained variance as we add additional components to the PCA? How does that number of components compare to the diminishing returns for accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Erase this and put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"Committing Part 6\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Bonus exercise: visualizing the decision boundaries for a portion of the feature space (2 *bonus* points)\n",
    "\n",
    "As you might imagine, visualizing decision boundaries with for a multidimensional feature space can be a challenge! That said, when trying to build some intuition about how these classifiers work, visualing 2D decisions boundaries can be useful.\n",
    "\n",
    "To earn some _extra points_ on this assignment try using the [following example](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html) as a guide to visualize the decision boundary for your \"best estimator\" parameters using your **2 PCA components** as your training features. **To be clear, you should be using your PCA component data and your best fit parameters, you should not just be running the example!** You should be able to get a plot that looks something like this:\n",
    "\n",
    "<img src=\"https://i.ibb.co/wL4xHGb/pca-boundaries.png\" alt=\"pca-boundaries\" border=\"0\">\n",
    "\n",
    "Since we didn't explicitly cover this in class, **you do not have to complete this part of the assignment unless you would like the extra credit points**.\n",
    "\n",
    "**&#9989; Task 7.1 (2 *extra* points):** Try to create a plot of the decision boundaries for the 2 principle components using your \"best estimator\" parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment wrap-up¶\n",
    "Please fill out the form that appears when you run the code below. **You must completely fill this out in order to receive credit for the assignment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\n",
    "\"\"\"\n",
    "<iframe \n",
    "\tsrc=\"https://forms.office.com/Pages/ResponsePage.aspx?id=MHEXIi9k2UGSEXQjetVofddd5T-Pwn1DlT6_yoCyuCFUNFFCRjgzN0JOTUFJQVNLR0VMQUZNNlVCTy4u\" \n",
    "\twidth=\"800px\" \n",
    "\theight=\"600px\" \n",
    "\tframeborder=\"0\" \n",
    "\tmarginheight=\"0\" \n",
    "\tmarginwidth=\"0\">\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you're done!\n",
    "Submit this assignment by uploading it to the course Desire2Learn web page. Go to the \"Homework Assignments\" folder, find the submission folder for Homework #5, and upload your notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
